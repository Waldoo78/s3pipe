# %% [markdown]
# # **Time Series Forecasting Models Analysis**

# %%
import os
import numpy as np
import pandas as pd
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.cluster.hierarchy import linkage, leaves_list
from scipy.spatial.distance import pdist


# %% [markdown]
# # *Datasets Introduction*

# %% [markdown]
# | Dataset | Source | Domain | Frequency | # Series | Avg | Min | Max | # Obs | Target Variates | Pred Length(S) | Windows | Pred Length(M) | Windows | Pred Length(L) | Windows |
# |---------|---------|---------|-----------|----------|-----|-----|-----|-------|----------------|----------------|---------|----------------|---------|----------------|---------|
# | Jena Weather | Autoformer (Wu et al., 2021) | Nature | 10T | 1 | 52,704 | 52,704 | 52,704 | 52,704 | 21 | 48 | 20 | 480 | 11 | 720 | 8 |
# | Jena Weather | Autoformer (Wu et al., 2021) | Nature | H | 1 | 8,784 | 8,784 | 8,784 | 8,784 | 21 | 48 | 19 | 480 | 2 | 720 | 2 |
# | Jena Weather | Autoformer (Wu et al., 2021) | Nature | D | 1 | 366 | 366 | 366 | 366 | 21 | 30 | 2 | | | | |
# | BizITObs - Application | AutoMixer (Palaskar et al., 2024) | Web/CloudOps | 10S | 1 | 8,834 | 8,834 | 8,834 | 8,834 | 2 | 60 | 15 | 600 | 2 | 900 | 1 |
# | BizITObs - Service | AutoMixer (Palaskar et al., 2024) | Web/CloudOps | 10S | 21 | 8,835 | 8,835 | 8,835 | 185,535 | 2 | 60 | 15 | 600 | 2 | 900 | 1 |
# | BizITObs - L2C | AutoMixer (Palaskar et al., 2024) | Web/CloudOps | 5T | 1 | 31,968 | 31,968 | 31,968 | 31,968 | 7 | 48 | 20 | 480 | 7 | 720 | 5 |
# | BizITObs - L2C | AutoMixer (Palaskar et al., 2024) | Web/CloudOps | H | 1 | 2,664 | 2,664 | 2,664 | 2,664 | 7 | 48 | 6 | 480 | 1 | 720 | 1 |
# | Bitbrains - Fast Storage | Grid Workloads Archive (Shen et al., 2015) | Web/CloudOps | 5T | 1,250 | 8,640 | 8,640 | 8,640 | 10,800,000 | 2 | 48 | 18 | 480 | 2 | 720 | 2 |
# | Bitbrains - Fast Storage | Grid Workloads Archive (Shen et al., 2015) | Web/CloudOps | H | 1,250 | 721 | 721 | 721 | 901,250 | 2 | 48 | 2 | | | | |
# | Bitbrains - rnd | Grid Workloads Archive (Shen et al., 2015) | Web/CloudOps | 5T | 500 | 8,640 | 8,640 | 8,640 | 4,320,000 | 2 | 48 | 18 | 480 | 2 | 720 | 2 |
# | Bitbrains - rnd | Grid Workloads Archive (Shen et al., 2015) | Web/CloudOps | H | 500 | 720 | 720 | 720 | 360,000 | 2 | 48 | 2 | | | | |
# | Restaurant | Recruit Rest. Comp. (Howard et al., 2017) | Sales | D | 807 | 358 | 67 | 478 | 289,303 | 1 | 30 | 1 | | | | |
# | ETT1 | Informer (Zhou et al., 2020) | Energy | 15T | 1 | 69,680 | 69,680 | 69,680 | 69,680 | 7 | 48 | 20 | 480 | 15 | 720 | 10 |
# | ETT1 | Informer (Zhou et al., 2020) | Energy | H | 1 | 17,420 | 17,420 | 17,420 | 17,420 | 7 | 48 | 20 | 480 | 4 | 720 | 3 |
# | ETT1 | Informer (Zhou et al., 2020) | Energy | D | 1 | 725 | 725 | 725 | 725 | 7 | 30 | 3 | | | | |
# | ETT1 | Informer (Zhou et al., 2020) | Energy | W-THU | 1 | 103 | 103 | 103 | 103 | 7 | 8 | 2 | | | | |
# | ETT2 | Informer (Zhou et al., 2020) | Energy | 15T | 1 | 69,680 | 69,680 | 69,680 | 69,680 | 7 | 48 | 20 | 480 | 15 | 720 | 10 |
# | ETT2 | Informer (Zhou et al., 2020) | Energy | H | 1 | 17,420 | 17,420 | 17,420 | 17,420 | 7 | 48 | 20 | 480 | 4 | 720 | 3 |
# | ETT2 | Informer (Zhou et al., 2020) | Energy | D | 1 | 725 | 725 | 725 | 725 | 7 | 30 | 3 | | | | |
# | ETT2 | Informer (Zhou et al., 2020) | Energy | W-THU | 1 | 103 | 103 | 103 | 103 | 7 | 8 | 2 | | | | |
# | Loop Seattle | LibCity (Wang et al., 2023a) | Transport | 5T | 323 | 105,120 | 105,120 | 105,120 | 33,953,760 | 1 | 48 | 20 | 480 | 20 | 720 | 15 |
# | Loop Seattle | LibCity (Wang et al., 2023a) | Transport | H | 323 | 8,760 | 8,760 | 8,760 | 2,829,480 | 1 | 48 | 19 | 480 | 2 | 720 | 2 |
# | Loop Seattle | LibCity (Wang et al., 2023a) | Transport | D | 323 | 365 | 365 | 365 | 117,895 | 1 | 30 | 2 | | | | |
# | SZ-Taxi | LibCity (Wang et al., 2023a) | Transport | 15T | 156 | 2,976 | 2,976 | 2,976 | 464,256 | 1 | 48 | 7 | 480 | 1 | 720 | 1 |
# | SZ-Taxi | LibCity (Wang et al., 2023a) | Transport | H | 156 | 744 | 744 | 744 | 116,064 | 1 | 48 | 2 | | | | |
# | M_DENSE | LibCity (Wang et al., 2023a) | Transport | H | 30 | 17,520 | 17,520 | 17,520 | 525,600 | 1 | 48 | 20 | 480 | 4 | 720 | 3 |
# | M_DENSE | LibCity (Wang et al., 2023a) | Transport | D | 30 | 730 | 730 | 730 | 21,900 | 1 | 30 | 3 | | | | |
# | Solar | LSTNet (Lai et al., 2017) | Energy | 10T | 137 | 52,560 | 52,560 | 52,560 | 7,200,720 | 1 | 48 | 20 | 480 | 11 | 720 | 8 |
# | Solar | LSTNet (Lai et al., 2017) | Energy | H | 137 | 8,760 | 8,760 | 8,760 | 1,200,120 | 1 | 48 | 19 | 480 | 2 | 720 | 2 |
# | Solar | LSTNet (Lai et al., 2017) | Energy | D | 137 | 365 | 365 | 365 | 50,005 | 1 | 30 | 2 | | | | |
# | Solar | LSTNet (Lai et al., 2017) | Energy | W-FRI | 137 | 52 | 52 | 52 | 7,124 | 1 | 8 | 1 | | | | |
# | Hierarchical Sales | Mancuso et al. (2020) | Sales | D | 118 | 1,825 | 1,825 | 1,825 | 215,350 | 1 | 30 | 7 | | | | |
# | Hierarchical Sales | Mancuso et al. (2020) | Sales | W-WED | 118 | 260 | 260 | 260 | 30,680 | 1 | 8 | 4 | | | | |
# | M4 Yearly | Monash (Godahewa et al., 2021) | Econ/Fin | A-DEC | 22,974 | 37 | 19 | 284 | 845,109 | 1 | 6 | 1 | | | | |
# | M4 Quarterly | Monash (Godahewa et al., 2021) | Econ/Fin | Q-DEC | 24,000 | 100 | 24 | 874 | 2,406,108 | 1 | 8 | 1 | | | | |
# | M4 Monthly | Monash (Godahewa et al., 2021) | Econ/Fin | M | 48,000 | 234 | 60 | 2,812 | 11,246,411 | 1 | 18 | 1 | | | | |
# | M4 Weekly | Monash (Godahewa et al., 2021) | Econ/Fin | W-SUN | 359 | 1,035 | 93 | 2,610 | 371,579 | 1 | 13 | 1 | | | | |
# | M4 Daily | Monash (Godahewa et al., 2021) | Econ/Fin | D | 4,227 | 2,371 | 107 | 9,933 | 10,023,836 | 1 | 14 | 1 | | | | |
# | M4 Hourly | Monash (Godahewa et al., 2021) | Econ/Fin | H | 414 | 902 | 748 | 1,008 | 373,372 | 1 | 48 | 2 | | | | |
# | Hospital | Monash (Godahewa et al., 2021) | Healthcare | M | 767 | 84 | 84 | 84 | 64,428 | 1 | 12 | 1 | | | | |
# | COVID Deaths | Monash (Godahewa et al., 2021) | Healthcare | D | 266 | 212 | 212 | 212 | 56,392 | 1 | 30 | 1 | | | | |
# | US Births | Monash (Godahewa et al., 2021) | Healthcare | D | 1 | 7,305 | 7,305 | 7,305 | 7,305 | 1 | 30 | 20 | | | | |
# | US Births | Monash (Godahewa et al., 2021) | Healthcare | W-TUE | 1 | 1,043 | 1,043 | 1,043 | 1,043 | 1 | 8 | 14 | | | | |
# | US Births | Monash (Godahewa et al., 2021) | Healthcare | M | 1 | 240 | 240 | 240 | 240 | 1 | 12 | 2 | | | | |
# | Saugeen | Monash (Godahewa et al., 2021) | Nature | D | 1 | 23,741 | 23,741 | 23,741 | 23,741 | 1 | 30 | 20 | | | | |
# | Saugeen | Monash (Godahewa et al., 2021) | Nature | W-THU | 1 | 3,391 | 3,391 | 3,391 | 3,391 | 1 | 8 | 20 | | | | |
# | Saugeen | Monash (Godahewa et al., 2021) | Nature | M | 1 | 780 | 780 | 780 | 780 | 1 | 12 | 7 | | | | |
# | Temperature Rain | Monash (Godahewa et al., 2021) | Nature | D | 32,072 | 725 | 725 | 725 | 23,252,400 | 1 | 30 | 3 | | | | |
# | KDD Cup 2018 | Monash (Godahewa et al., 2021) | Nature | H | 270 | 10,898 | 9,504 | 10,920 | 2,942,364 | 1 | 48 | 20 | 480 | 2 | 720 | 2 |
# | KDD Cup 2018 | Monash (Godahewa et al., 2021) | Nature | D | 270 | 455 | 396 | 455 | 122,791 | 1 | 30 | 2 | | | | |
# | Car Parts | Monash (Godahewa et al., 2021) | Sales | M | 2,674 | 51 | 51 | 51 | 136,374 | 1 | 12 | 1 | | | | |
# | Electricity | UCI ML Archive (Trindade, 2015) | Energy | 15T | 370 | 140,256 | 140,256 | 140,256 | 51,894,720 | 1 | 48 | 20 | 480 | 20 | 720 | 20 |
# | Electricity | UCI ML Archive (Trindade, 2015) | Energy | H | 370 | 35,064 | 35,064 | 35,064 | 12,973,680 | 1 | 48 | 20 | 480 | 8 | 720 | 5 |
# | Electricity | UCI ML Archive (Trindade, 2015) | Energy | D | 370 | 1,461 | 1,461 | 1,461 | 540,570 | 1 | 30 | 5 | | | | |
# | Electricity | UCI ML Archive (Trindade, 2015) | Energy | W-FRI | 370 | 208 | 208 | 208 | 76,960 | 1 | 8 | 3 | | | | |

# %%
gift_results_folder='/home/wbouainouche/Samformer_distillation/GIFT-Eval/results'

# %%
models = {}
for model in os.listdir(gift_results_folder):
    if model != 'MLP':
        if os.path.isdir(os.path.join(gift_results_folder, model)):
            csv_path = os.path.join(gift_results_folder, model, 'all_results.csv')
            if os.path.exists(csv_path):
                df = pd.read_csv(csv_path)
                models[model] = df.sort_values('dataset').reset_index(drop=True)

# %%
domain_series = pd.Series(models['Seasonal_naive']['domain'])
num_variates_series = pd.Series(models['Seasonal_naive']['num_variates'])

# %% [markdown]
# # Ratio with naive baseline 

# %%
# MASE ratios vs Seasonal_naive
all_ratios_MASE_baseline = {}
for model in models.keys():
 ratios_b = models[model]["eval_metrics/MASE[0.5]"] / models['Seasonal_naive']["eval_metrics/MASE[0.5]"]
 all_ratios_MASE_baseline[model] = ratios_b-1

df_ratios_MASE_baseline = pd.DataFrame(all_ratios_MASE_baseline)
df_ratios_MASE_baseline['domain'] = domain_series.values
df_ratios_MASE_baseline['num_variates'] = num_variates_series.values
df_ratios_MASE_baseline.index = models['Seasonal_naive']['dataset']  
df_ratios_MASE_baseline['frequency'] = df_ratios_MASE_baseline.index.str.split('/').str[1]
df_ratios_MASE_baseline['term'] = df_ratios_MASE_baseline.index.str.split('/').str[2]

model_columns_MASE_baseline = [col for col in df_ratios_MASE_baseline.columns if col not in ['domain', 'num_variates', 'frequency', 'term']]
df_ranks_MASE_baseline = df_ratios_MASE_baseline[model_columns_MASE_baseline].rank(axis=1, method='min')

# CRPS ratios vs Seasonal_naive
all_ratios_CRPS_baseline = {}
for model in models.keys():
 ratios_b = models[model]["eval_metrics/mean_weighted_sum_quantile_loss"] / models['Seasonal_naive']["eval_metrics/mean_weighted_sum_quantile_loss"]
 all_ratios_CRPS_baseline[model] = ratios_b-1

df_ratios_CRPS_baseline = pd.DataFrame(all_ratios_CRPS_baseline)
df_ratios_CRPS_baseline['domain'] = domain_series.values
df_ratios_CRPS_baseline['num_variates'] = num_variates_series.values
df_ratios_CRPS_baseline.index = models['Seasonal_naive']['dataset']  
df_ratios_CRPS_baseline['frequency'] = df_ratios_CRPS_baseline.index.str.split('/').str[1]
df_ratios_CRPS_baseline['term'] = df_ratios_CRPS_baseline.index.str.split('/').str[2]

model_columns_CRPS_baseline = [col for col in df_ratios_CRPS_baseline.columns if col not in ['domain', 'num_variates', 'frequency', 'term']]
df_ranks_CRPS_baseline = df_ratios_CRPS_baseline[model_columns_CRPS_baseline].rank(axis=1, method='min')

# %% [markdown]
# # Ratio with Linear++ baseline

# %%
# MASE ratios vs Linear++
all_ratios_MASE_linear = {}
for model in models.keys():
   ratios_MASE_l = models[model]["eval_metrics/MASE[0.5]"] / models["Linear++"]["eval_metrics/MASE[0.5]"]
   all_ratios_MASE_linear[model] = ratios_MASE_l - 1  

df_ratios_MASE_linear = pd.DataFrame(all_ratios_MASE_linear)
df_ratios_MASE_linear['domain'] = domain_series.values
df_ratios_MASE_linear['num_variates'] = num_variates_series.values
df_ratios_MASE_linear.index = models['Seasonal_naive']['dataset']
df_ratios_MASE_linear['frequency'] = df_ratios_MASE_linear.index.str.split('/').str[1]
df_ratios_MASE_linear['term'] = df_ratios_MASE_linear.index.str.split('/').str[2]

model_columns_MASE_linear = [col for col in df_ratios_MASE_linear.columns if col not in ['domain', 'num_variates', 'frequency', 'term']]
df_ranks_MASE_linear = df_ratios_MASE_linear[model_columns_MASE_linear].rank(axis=1, method='min')

# CRPS ratios vs Linear++
all_ratios_CRPS_linear = {}
for model in models.keys():
   ratios_CRPS_l = models[model]["eval_metrics/mean_weighted_sum_quantile_loss"] / models["Linear++"]["eval_metrics/mean_weighted_sum_quantile_loss"]
   all_ratios_CRPS_linear[model] = ratios_CRPS_l - 1  

df_ratios_CRPS_linear = pd.DataFrame(all_ratios_CRPS_linear)
df_ratios_CRPS_linear['domain'] = domain_series.values
df_ratios_CRPS_linear['num_variates'] = num_variates_series.values
df_ratios_CRPS_linear.index = models['Seasonal_naive']['dataset']
df_ratios_CRPS_linear['frequency'] = df_ratios_CRPS_linear.index.str.split('/').str[1]
df_ratios_CRPS_linear['term'] = df_ratios_CRPS_linear.index.str.split('/').str[2]

model_columns_CRPS_linear = [col for col in df_ratios_CRPS_linear.columns if col not in ['domain', 'num_variates', 'frequency', 'term']]
df_ranks_CRPS_linear = df_ratios_CRPS_linear[model_columns_CRPS_linear].rank(axis=1, method='min')

# %%
# Performance analysis for MASE vs Seasonal_naive
scores_geom_baseline_mase = {}
mean_ranks_baseline_mase = {}
median_ranks_baseline_mase = {}
worse_than_baseline_mase = {}

for model in model_columns_MASE_baseline:
    ratios_baseline_mase = df_ratios_MASE_baseline[model]
    ranks_baseline_mase = df_ranks_MASE_baseline[model]
    
    valid_ratios_mase = ratios_baseline_mase + 1 
    scores_geom_baseline_mase[model] = np.exp(np.mean(np.log(valid_ratios_mase))) - 1
    mean_ranks_baseline_mase[model] = ranks_baseline_mase.mean()
    median_ranks_baseline_mase[model] = ranks_baseline_mase.median()
    worse_than_baseline_mase[model] = (ratios_baseline_mase > 0).sum()

performance_data_baseline_mase = []
for i, (model, score) in enumerate(sorted(scores_geom_baseline_mase.items(), key=lambda x: x[1]), 1):
    performance_data_baseline_mase.append({
        'Rank': i,
        'Model': model,
        'Geom_Score_Seasonal_Naive_Baseline': round(scores_geom_baseline_mase[model] + 1, 3),
        'Mean_Rank_Seasonal_Naive_Baseline': round(mean_ranks_baseline_mase[model], 1),
        'Median_Rank_Seasonal_Naive_Baseline': round(median_ranks_baseline_mase[model], 1),
        'Worse_Count_Seasonal_Naive_Baseline': int(worse_than_baseline_mase[model]),
        'Fail_Rate_Seasonal_Naive_Baseline': f"{worse_than_baseline_mase[model] / len(df_ratios_MASE_baseline):.1%}"
    })

df_performance_baseline_mase = pd.DataFrame(performance_data_baseline_mase)
print("MASE Performance vs Seasonal Naive Baseline:")
print(df_performance_baseline_mase.to_string(index=False))


# %%
# Performance analysis for CRPS vs Seasonal_naive
scores_geom_baseline_crps = {}
mean_ranks_baseline_crps = {}
median_ranks_baseline_crps = {}
worse_than_baseline_crps = {}

for model in model_columns_CRPS_baseline:
    ratios_baseline_crps = df_ratios_CRPS_baseline[model]
    ranks_baseline_crps = df_ranks_CRPS_baseline[model]

    valid_ratios_crps = ratios_baseline_crps + 1 
    scores_geom_baseline_crps[model] = np.exp(np.mean(np.log(valid_ratios_crps))) - 1

    mean_ranks_baseline_crps[model] = ranks_baseline_crps.mean()
    median_ranks_baseline_crps[model] = ranks_baseline_crps.median()
    
    worse_than_baseline_crps[model] = (ratios_baseline_crps > 0).sum()

performance_data_baseline_crps = []
for i, (model, score) in enumerate(sorted(scores_geom_baseline_crps.items(), key=lambda x: x[1]), 1):
    performance_data_baseline_crps.append({
        'Rank': i,
        'Model': model,
        'Geom_Score_Seasonal_Naive_Baseline': round(scores_geom_baseline_crps[model] + 1, 3),  
        'Mean_Rank_Seasonal_Naive_Baseline': round(mean_ranks_baseline_crps[model], 1),
        'Median_Rank_Seasonal_Naive_Baseline': round(median_ranks_baseline_crps[model], 1),
        'Worse_Count_Seasonal_Naive_Baseline': int(worse_than_baseline_crps[model]),
        'Fail_Rate_Seasonal_Naive_Baseline': f"{worse_than_baseline_crps[model] / len(df_ratios_CRPS_baseline):.1%}"
    })

df_performance_baseline_crps = pd.DataFrame(performance_data_baseline_crps)
print("CRPS Performance vs Seasonal Naive Baseline:")
print(df_performance_baseline_crps.to_string(index=False))

# %%
# Performance analysis for MASE vs Linear++
scores_geom_linear_mase = {}
mean_ranks_linear_mase = {}
median_ranks_linear_mase = {}
worse_than_linear_mase = {}

for model in model_columns_MASE_linear:
    ratios_linear_mase = df_ratios_MASE_linear[model]
    ranks_linear_mase = df_ranks_MASE_linear[model]
    
    valid_ratios_mase = ratios_linear_mase + 1 
    scores_geom_linear_mase[model] = np.exp(np.mean(np.log(valid_ratios_mase))) - 1
    mean_ranks_linear_mase[model] = ranks_linear_mase.mean()
    median_ranks_linear_mase[model] = ranks_linear_mase.median()
    worse_than_linear_mase[model] = (ratios_linear_mase > 0).sum()

performance_data_linear_mase = []
for i, (model, score) in enumerate(sorted(scores_geom_linear_mase.items(), key=lambda x: x[1]), 1):
    performance_data_linear_mase.append({
        'Rank': i,
        'Model': model,
        'Geom_Score_Linear_Baseline': round(scores_geom_linear_mase[model] + 1, 3),
        'Mean_Rank_Linear_Baseline': round(mean_ranks_linear_mase[model], 1),
        'Median_Rank_Linear_Baseline': round(median_ranks_linear_mase[model], 1),
        'Worse_Count_Linear_Baseline': int(worse_than_linear_mase[model]),
        'Fail_Rate_Linear_Baseline': f"{worse_than_linear_mase[model] / len(df_ratios_MASE_linear):.1%}"
    })

df_performance_linear_mase = pd.DataFrame(performance_data_linear_mase)
print("MASE Performance vs Linear++ Baseline:")
print(df_performance_linear_mase.to_string(index=False))

# %%
# Performance analysis for CRPS vs Linear++
scores_geom_linear_crps = {}
mean_ranks_linear_crps = {}
median_ranks_linear_crps = {}
worse_than_linear_crps = {}

for model in model_columns_CRPS_linear:
    ratios_linear_crps = df_ratios_CRPS_linear[model]
    ranks_linear_crps = df_ranks_CRPS_linear[model]

    valid_ratios_crps = ratios_linear_crps + 1 
    scores_geom_linear_crps[model] = np.exp(np.mean(np.log(valid_ratios_crps))) - 1

    mean_ranks_linear_crps[model] = ranks_linear_crps.mean()
    median_ranks_linear_crps[model] = ranks_linear_crps.median()
    
    worse_than_linear_crps[model] = (ratios_linear_crps > 0).sum()

performance_data_linear_crps = []
for i, (model, score) in enumerate(sorted(scores_geom_linear_crps.items(), key=lambda x: x[1]), 1):
    performance_data_linear_crps.append({
        'Rank': i,
        'Model': model,
        'Geom_Score_Linear_Baseline': round(scores_geom_linear_crps[model] + 1, 3),  
        'Mean_Rank_Linear_Baseline': round(mean_ranks_linear_crps[model], 1),
        'Median_Rank_Linear_Baseline': round(median_ranks_linear_crps[model], 1),
        'Worse_Count_Linear_Baseline': int(worse_than_linear_crps[model]),
        'Fail_Rate_Linear_Baseline': f"{worse_than_linear_crps[model] / len(df_ratios_CRPS_linear):.1%}"
    })

df_performance_linear_crps = pd.DataFrame(performance_data_linear_crps)
print("CRPS Performance vs Linear++ Baseline:")
print(df_performance_linear_crps.to_string(index=False))

# %% [markdown]
# # Dataset Analysis

# %%
# Heatmap MASE vs Seasonal_naive baseline
heatmap_data_MASE_baseline = df_ratios_MASE_baseline.drop(columns=[ 'domain', 'num_variates', 'frequency', 'term', ])

model_order_MASE_b = list(dict(sorted(scores_geom_baseline_mase.items(), key=lambda item: item[1])).keys())[::-1]
dataset_order_MASE_b = (np.exp(np.log(heatmap_data_MASE_baseline + 1).mean(axis=1)) - 1).sort_values(ascending=False).index

heatmap_sorted_MASE_baseline = heatmap_data_MASE_baseline.loc[dataset_order_MASE_b, model_order_MASE_b]

plt.figure(figsize=(20, 25))
sns.heatmap(heatmap_sorted_MASE_baseline, 
           annot=False,
           cmap='seismic',
           center=0,
           vmax=1,
           vmin=-1)

plt.title('MASE Ratios Heatmap (Reference: Seasonal Naive)', fontsize=16)
plt.xticks(rotation=45, ha='right')

plt.savefig("heatmap_MASE_baseline.pdf", format='pdf', bbox_inches='tight')
plt.show()


# %%
# Heatmap CRPS vs Seasonal_naive baseline
heatmap_data_CRPS_baseline = df_ratios_CRPS_baseline.drop(columns=[ 'domain', 'num_variates', 'frequency', 'term', ])

model_order_CRPS_b = list(dict(sorted(scores_geom_baseline_crps.items(), key=lambda item: item[1])).keys())[::-1]
dataset_order_CRPS_b = (np.exp(np.log(heatmap_data_CRPS_baseline + 1).mean(axis=1)) - 1).sort_values(ascending=False).index

heatmap_sorted_CRPS_baseline = heatmap_data_CRPS_baseline.loc[dataset_order_CRPS_b, model_order_CRPS_b]

plt.figure(figsize=(20, 25))
sns.heatmap(heatmap_sorted_CRPS_baseline, 
           annot=False,
           cmap='seismic',
           center=0,
           vmax=1,
           vmin=-1)

plt.title('CRPS Ratios Heatmap (Reference: Seasonal Naive)')
plt.xticks(rotation=45, ha='right')


plt.savefig("heatmap_CRPS_baseline.pdf", format='pdf', bbox_inches='tight')
plt.show()


# %%
# Heatmap MASE vs Linear++ baseline
heatmap_data_MASE_linear = df_ratios_MASE_linear.drop(columns=[ 'domain', 'num_variates', 'frequency', 'term', ])

model_order_MASE_linear = list(dict(sorted(scores_geom_linear_mase.items(), key=lambda item: item[1])).keys())[::-1]
dataset_order_MASE_linear = (np.exp(np.log(heatmap_data_MASE_linear + 1).mean(axis=1)) - 1).sort_values(ascending=False).index

heatmap_sorted_MASE_linear = heatmap_data_MASE_linear.loc[dataset_order_MASE_linear, model_order_MASE_linear]

plt.figure(figsize=(20, 25))
sns.heatmap(heatmap_sorted_MASE_linear, 
           annot=False,
           cmap='seismic',
           center=0,
           vmax=1,
           vmin=-1)

plt.title('MASE Ratios Heatmap (Reference: Linear++)', fontsize=16)
plt.xticks(rotation=45, ha='right')


plt.savefig("heatmap_MASE_linear.pdf", format='pdf', bbox_inches='tight')
plt.show()


# %%
# Heatmap CRPS vs Linear++ baseline
heatmap_data_CRPS_linear = df_ratios_CRPS_linear.drop(columns=[ 'domain', 'num_variates', 'frequency', 'term', ])

model_order_CRPS_linear = list(dict(sorted(scores_geom_linear_crps.items(), key=lambda item: item[1])).keys())[::-1]
dataset_order_CRPS_linear = (np.exp(np.log(heatmap_data_CRPS_linear + 1).mean(axis=1)) - 1).sort_values(ascending=False).index

heatmap_sorted_CRPS_linear = heatmap_data_CRPS_linear.loc[dataset_order_CRPS_linear, model_order_CRPS_linear]

plt.figure(figsize=(20, 25))
sns.heatmap(heatmap_sorted_CRPS_linear, 
           annot=False,
           cmap='seismic',
           center=0,
           vmax=1,
           vmin=-1)

plt.title('CRPS Ratios Heatmap (Reference: Linear++)', fontsize=16)
plt.xticks(rotation=45, ha='right')

plt.savefig("heatmap_CRPS_linear.pdf", format='pdf', bbox_inches='tight')
plt.show()

# %% [markdown]
# ## Dataset Difficulty Ranking
# 
# **Metric Definition**: Dataset difficulty is measured as the arithmetic mean of normalized MASE across all models.

# %%
# Dataset difficulty MASE vs Seasonal_naive
dataset_difficulty_MASE_geom_b = {}

for dataset in heatmap_sorted_MASE_baseline.index:
   row = heatmap_sorted_MASE_baseline.loc[dataset]
   dataset_difficulty_MASE_geom_b[dataset] = np.exp(np.mean(np.log(row + 1))) - 1

sorted_difficulty_MASE_b = sorted(dataset_difficulty_MASE_geom_b.items(), key=lambda x: x[1], reverse=True)

data_MASE_b = []
for i, (dataset, geom_score) in enumerate(sorted_difficulty_MASE_b, 1):
   row = heatmap_sorted_MASE_baseline.loc[dataset]
   geom_std = np.exp(np.std(np.log(row + 1))) - 1
   data_MASE_b.append({
       'Rank': i,
       'Dataset': dataset,
       'Geom_Mean': round(geom_score+1, 3),
       'Geom_Std': round(geom_std, 3)
   })

df_difficulty_MASE_b = pd.DataFrame(data_MASE_b)

print("Dataset Difficulty Ranking MASE: Seasonal naive baseline")
print("=" * 70)
print(df_difficulty_MASE_b.to_string(index=False))
print(f"\nTotal datasets: {len(sorted_difficulty_MASE_b)}")


# %%
# Dataset difficulty CRPS vs Seasonal_naive
dataset_difficulty_CRPS_geom_b = {}

for dataset in heatmap_sorted_CRPS_baseline.index:
    row = heatmap_sorted_CRPS_baseline.loc[dataset]
    dataset_difficulty_CRPS_geom_b[dataset] = np.exp(np.mean(np.log(row + 1))) - 1

sorted_difficulty_CRPS_b = sorted(dataset_difficulty_CRPS_geom_b.items(), key=lambda x: x[1], reverse=True)

data_CRPS_b = []
for i, (dataset, geom_score) in enumerate(sorted_difficulty_CRPS_b, 1):
    row = heatmap_sorted_CRPS_baseline.loc[dataset]
    geom_std = np.exp(np.std(np.log(row + 1))) - 1
    data_CRPS_b.append({
        'Rank': i,
        'Dataset': dataset,
        'Geom_Mean': round(geom_score+1, 3),
        'Geom_Std': round(geom_std, 3)
    })

df_difficulty_CRPS_b = pd.DataFrame(data_CRPS_b)

print("Dataset Difficulty Ranking CRPS: Seasonal naive baseline")
print("=" * 70)
print(df_difficulty_CRPS_b.to_string(index=False))
print(f"\nTotal datasets: {len(sorted_difficulty_CRPS_b)}")

# %%
# Dataset difficulty MASE vs Linear++
dataset_difficulty_MASE_geom_l = {}

for dataset in heatmap_sorted_MASE_linear.index:
   row = heatmap_sorted_MASE_linear.loc[dataset]
   dataset_difficulty_MASE_geom_l[dataset] = np.exp(np.mean(np.log(row + 1))) - 1

sorted_difficulty_MASE_l = sorted(dataset_difficulty_MASE_geom_l.items(), key=lambda x: x[1], reverse=True)

data_MASE_l = []
for i, (dataset, geom_score) in enumerate(sorted_difficulty_MASE_l, 1):
   row = heatmap_sorted_MASE_linear.loc[dataset]
   geom_std = np.exp(np.std(np.log(row + 1))) - 1
   data_MASE_l.append({
       'Rank': i,
       'Dataset': dataset,
       'Geom_Mean': round(geom_score+1, 3),
       'Geom_Std': round(geom_std, 3)
   })

df_difficulty_MASE_l = pd.DataFrame(data_MASE_l)

print("Dataset Difficulty Ranking MASE: Linear++ baseline")
print("=" * 70)
print(df_difficulty_MASE_l.to_string(index=False))
print(f"\nTotal datasets: {len(sorted_difficulty_MASE_l)}")

# %%
# Dataset difficulty CRPS vs Linear++
dataset_difficulty_CRPS_geom_l = {}

for dataset in heatmap_sorted_CRPS_linear.index:
   row = heatmap_sorted_CRPS_linear.loc[dataset]
   dataset_difficulty_CRPS_geom_l[dataset] = np.exp(np.mean(np.log(row + 1))) - 1

sorted_difficulty_CRPS_l = sorted(dataset_difficulty_CRPS_geom_l.items(), key=lambda x: x[1], reverse=True)

data_CRPS_l = []
for i, (dataset, geom_score) in enumerate(sorted_difficulty_CRPS_l, 1):
   row = heatmap_sorted_CRPS_linear.loc[dataset]
   geom_std = np.exp(np.std(np.log(row + 1))) - 1
   data_CRPS_l.append({
       'Rank': i,
       'Dataset': dataset,
       'Geom_Mean': round(geom_score+1, 3),
       'Geom_Std': round(geom_std, 3)
   })

df_difficulty_CRPS_l = pd.DataFrame(data_CRPS_l)

print("Dataset Difficulty Ranking CRPS: Linear++ baseline")
print("=" * 70)
print(df_difficulty_CRPS_l.to_string(index=False))
print(f"\nTotal datasets: {len(sorted_difficulty_CRPS_l)}")

# %% [markdown]
# # Temporal Evolution Analysis
# 
# ## Model Categories by Publication Year
# 
# | Year | Count | Models |
# |------|-------|--------|
# | 2015 | 5 | naive, seasonal_naive, auto_theta, auto_ets, auto_arima |
# | 2017 | 1 | deepar |
# | 2019 | 1 | tft |
# | 2020 | 1 | N-BEATS |
# | 2022 | 2 | PatchTST, DLinear |
# | 2023 | 1 | tide |
# | 2024 | 22 | YingLong_6m, YingLong_50m, YingLong_110m, YingLong_300m, tabpfn_ts, TTM-R1-Zeroshot, TTM-R2-Zeroshot, TTM-R2-Finetuned, Chronos_small, chronos_base, chronos_large, chronos_bolt_small, chronos_bolt_base, timesfm, timesfm_2_0_500m, Moirai_small, Moirai_base, Moirai_large, Lag-Llama, Timer, tempo_ensemble |
# | 2025 | 4 | TiRex, Toto_Open_Base_1.0, sundial_base_128m, visionts |
# 

# %%
import matplotlib.lines as mlines
models_years_analysis = {
  "<2019": ['Seasonal_naive'],
  2019: ['TFT'],
  2020: ['N-BEATS'],
  2022: ['PatchTST', 'DLinear'],
  2023: ['iTransformer', 'Lag-Llama'],
  2024: ['Chronos_bolt_small', 'Chronos_base', 'Moirai_small'],
  2025: ['TiRex', 'Linear++']
}

model_markers = {
  'Seasonal_naive': ('D', '#ff7f0e'),
  'DeepAR': ('D', '#1f77b4'),
  'TFT': ('D', '#1f77b4'),
  'N-BEATS': ('D', '#1f77b4'),
  'PatchTST': ('D', '#1f77b4'),
  'DLinear': ('D', '#1f77b4'),
  'Linear++': ('D', '#1f77b4'),
  'iTransformer': ('D', '#1f77b4'),
  'TabPFN-TS': ('o', '#d62728'),
  'Lag-Llama': ('o', '#d62728'),  
  'Chronos_base': ('o', '#d62728'),
  'Moirai_small': ('o', '#d62728'),
  'Chronos_bolt_small': ('o', '#d62728'),
  'TiRex': ('o', '#d62728'),
  'Toto_Open_Base_1.0': ('o', '#d62728'),
}

model_categories = {
  'Seasonal_naive': 'Statistical',
  'DeepAR': 'Deep Learning',
  'TFT': 'Deep Learning',
  'N-BEATS': 'Deep Learning',
  'PatchTST': 'Deep Learning',
  'DLinear': 'Deep Learning',
  'Linear++': 'Deep Learning',
  'iTransformer': 'Deep Learning',
  'TabPFN-TS': 'Zero-shot',
  'Chronos_bolt_small': 'Zero-shot',
  'TiRex': 'Zero-shot',
  'Chronos_base': 'Zero-shot',
  'Moirai_small': 'Zero-shot',
  'Lag-Llama': 'Zero-shot',
  'Toto_Open_Base_1.0': 'Zero-shot',
}

positions = [1, 2, 3, 4, 5, 6, 7]
labels = ["<2019", 2019, 2020, 2022, 2023, 2024, 2025]
year_to_pos = {year: pos for year, pos in zip(labels, positions)}

plt.figure(figsize=(12, 8))

plt.axhline(y=1.0, color='red', linestyle='--', alpha=0.5,
          label='Classical Baseline (MASE=1.0)')
plt.axhline(y=0.870, color='green', linestyle='--', alpha=0.5,
          label='Proposed baseline (MASE=0.87)')

plotted_categories = set()

for year, models in models_years_analysis.items():
  x = year_to_pos[year]
  for model in models:
      if model in scores_geom_baseline_mase:
          marker, color = model_markers[model]
          category = model_categories[model]
          score = scores_geom_baseline_mase[model] + 1

          label = category if category not in plotted_categories else ""
          if label:
              plotted_categories.add(category)

          if model == 'Linear++':
              plt.scatter(x, score, marker='*', s=300, color='green',
                          edgecolors='black', linewidth=1.5, alpha=0.9, label=label)
              plt.annotate(model, (x, score), xytext=(0, 12), textcoords='offset points',
                           ha='center', fontsize=12.5, fontweight='bold', alpha=0.9)
          else:
              plt.scatter(x, score, marker=marker, color=color, s=150,
                          alpha=0.8, edgecolors='black', linewidth=1, label=label)
              plt.annotate(model, (x, score), xytext=(0, 10),
                           textcoords='offset points', ha='center',
                           fontsize=12.5, alpha=0.8)

plt.xlabel('Year', fontsize=14, fontweight='bold')
plt.ylabel('Geometric Mean MASE Ratio vs Seasonal Naive', fontsize=14, fontweight='bold')
plt.title('Evolution of Time Series Forecasting Models Performance (MASE)', 
        fontsize=16, fontweight='bold', pad=20)

plt.grid(True, alpha=0.3)

plt.legend(handles=[
  mlines.Line2D([], [], color='#ff7f0e', marker='D', linestyle='None', markersize=10, label='Statistical'),
  mlines.Line2D([], [], color='#1f77b4', marker='D', linestyle='None', markersize=10, label='Deep Learning'),
  mlines.Line2D([], [], color='#d62728', marker='o', linestyle='None', markersize=10, label='Zero-shot'),
  mlines.Line2D([], [], color='red', linestyle='--', label='Classical baseline (MASE=1.0)'),
  mlines.Line2D([], [], color='green', linestyle='--', label='Proposed baseline (MASE=0.87)')
], loc='upper left', fontsize=10)

plt.xlim(0.5, 7.5)
plt.xticks(positions, labels)

plt.tight_layout()
plt.savefig("model_evolution_MASE.pdf", format='pdf', bbox_inches='tight')
plt.show()

# %% [markdown]
# # Domain and Frequency Analysis

# %%
# Performance by Domain - MASE vs Seasonal_naive
domain_performance_mase_b = {}
for domain in df_ratios_MASE_baseline['domain'].unique():
  domain_data = df_ratios_MASE_baseline[df_ratios_MASE_baseline['domain'] == domain]
  domain_performance_mase_b[domain] = np.exp(np.log(domain_data[model_columns_MASE_baseline] + 1).mean()) - 1

df_domain_mase_b = pd.DataFrame(domain_performance_mase_b).T
domain_order_mase_b = (np.exp(np.log(df_domain_mase_b + 1).mean(axis=1)) - 1).sort_values(ascending=False).index
df_domain_mase_b = df_domain_mase_b.loc[domain_order_mase_b, model_order_MASE_b]

plt.figure(figsize=(25, 6))
sns.heatmap(df_domain_mase_b, 
         annot=False, 
         cmap='seismic',
         center=0,
         vmin=-1,
         vmax=1)
plt.title('Performance by Domain - MASE: Seasonal Naive Baseline')
plt.xlabel('Models')
plt.ylabel('Domains')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()

plt.savefig("performance_domain_MASE_baseline.pdf", format='pdf', bbox_inches='tight')
plt.show()

# Performance by Domain - CRPS vs Seasonal_naive
domain_performance_crps_b = {}
for domain in df_ratios_CRPS_baseline['domain'].unique():
 domain_data = df_ratios_CRPS_baseline[df_ratios_CRPS_baseline['domain'] == domain]
 domain_performance_crps_b[domain] = np.exp(np.log(domain_data[model_columns_CRPS_baseline] + 1).mean()) - 1

df_domain_crps_b = pd.DataFrame(domain_performance_crps_b).T
domain_order_crps_b = (np.exp(np.log(df_domain_crps_b + 1).mean(axis=1)) - 1).sort_values(ascending=False).index
df_domain_crps_b = df_domain_crps_b.loc[domain_order_crps_b, model_order_CRPS_b]

plt.figure(figsize=(25, 6))
sns.heatmap(df_domain_crps_b, 
        annot=False, 
        cmap='seismic',
        center=0,
        vmin=-1,
        vmax=1)
plt.title('Performance by Domain - CRPS: Seasonal Naive Baseline', fontsize=30)
plt.xlabel('Models')
plt.ylabel('Domains')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()

plt.savefig("performance_domain_CRPS_baseline.pdf", format='pdf', bbox_inches='tight')
plt.show()


# %%
# Performance by Domain - MASE vs Linear++
domain_performance_mase_linear = {}
for domain in df_ratios_MASE_linear['domain'].unique():
  domain_data = df_ratios_MASE_linear[df_ratios_MASE_linear['domain'] == domain]
  domain_performance_mase_linear[domain] = np.exp(np.log(domain_data[model_columns_MASE_linear] + 1).mean()) - 1

df_domain_mase_linear = pd.DataFrame(domain_performance_mase_linear).T
domain_order_mase_linear = (np.exp(np.log(df_domain_mase_linear + 1).mean(axis=1)) - 1).sort_values(ascending=False).index
df_domain_mase_linear = df_domain_mase_linear.loc[domain_order_mase_linear, model_order_MASE_linear]

plt.figure(figsize=(25, 6))
sns.heatmap(df_domain_mase_linear, 
         annot=False, 
         cmap='seismic',
         center=0,
         vmin=-1,
         vmax=1)
plt.title('Performance by Domain - MASE vs Linear++ Baseline')
plt.xlabel('Models')
plt.ylabel('Domains')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()

plt.savefig("performance_domain_MASE_linear.pdf", format='pdf', bbox_inches='tight')
plt.show()

# Performance by Domain - CRPS vs Linear++
domain_performance_crps_linear = {}
for domain in df_ratios_CRPS_linear['domain'].unique():
  domain_data = df_ratios_CRPS_linear[df_ratios_CRPS_linear['domain'] == domain]
  domain_performance_crps_linear[domain] = np.exp(np.log(domain_data[model_columns_CRPS_linear] + 1).mean()) - 1

df_domain_crps_linear = pd.DataFrame(domain_performance_crps_linear).T
domain_order_crps_linear = (np.exp(np.log(df_domain_crps_linear + 1).mean(axis=1)) - 1).sort_values(ascending=False).index
df_domain_crps_linear = df_domain_crps_linear.loc[domain_order_crps_linear, model_order_CRPS_linear]

plt.figure(figsize=(25, 6))
sns.heatmap(df_domain_crps_linear, 
         annot=False, 
         cmap='seismic',
         center=0,
         vmin=-1,
         vmax=1)
plt.title('Performance by Domain - CRPS vs Linear++ Baseline')
plt.xlabel('Models')
plt.ylabel('Domains')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()

plt.savefig("performance_domain_CRPS_linear.pdf", format='pdf', bbox_inches='tight')
plt.show()



# %%
# Performance by Period (filtered datasets) - MASE vs Seasonal_naive
datasets_with_3_horizons = [
 'jena_weather/10T',
 'jena_weather/H', 
 'bizitobs_service/10S',
 'bizitobs_l2c/5T',
 'bitbrains_fast storage/5T',
 'bitbrains_rnd/5T',
 'ett1/15T',
 'ett1/H',
 'ett2/15T', 
 'ett2/H',
 'loop_seattle/5T',
 'loop_seattle/H',
 'sz_taxi/15T',
 'm_dense/H',
 'solar/10T',
 'solar/H',
 'kdd_cup_2018/H',
 'electricity/15T',
 'electricity/H'
]

datasets_filtered_complete = []
for dataset in datasets_with_3_horizons:
  for term in ['short', 'medium', 'long']:
      datasets_filtered_complete.append(f"{dataset}/{term}")

df_filtered_baseline = df_ratios_MASE_baseline[df_ratios_MASE_baseline.index.isin(datasets_filtered_complete)]

period_performance_filtered_baseline = {}
for period in df_filtered_baseline['term'].unique():
  period_data_filtered = df_filtered_baseline[df_filtered_baseline['term'] == period]
  period_performance_filtered_baseline[period] = np.exp(np.log(period_data_filtered[model_columns_MASE_baseline] + 1).mean()) - 1

df_period_filtered_baseline = pd.DataFrame(period_performance_filtered_baseline).T

model_order_period_filtered_baseline = (np.exp(np.log(df_period_filtered_baseline + 1).mean()) - 1).sort_values(ascending=False).index
period_order_filtered_baseline = (np.exp(np.log(df_period_filtered_baseline + 1).mean(axis=1)) - 1).sort_values(ascending=False).index

df_period_filtered_baseline = df_period_filtered_baseline.loc[period_order_filtered_baseline, model_order_period_filtered_baseline]

plt.figure(figsize=(25, 8))
sns.heatmap(df_period_filtered_baseline, annot=False, fmt='.2f', cmap='seismic', center=0, vmin=-1, vmax=1)
plt.title('Geometric Mean MASE by Period - filtered (Seasonal Naive baseline)')
plt.xlabel('Models')
plt.ylabel('Periods')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()

plt.savefig("performance_period_filtered_MASE_baseline.pdf", format='pdf', bbox_inches='tight')
plt.show()

# Performance by Period (filtered datasets) - MASE vs Linear++
df_filtered = df_ratios_MASE_linear[df_ratios_MASE_linear.index.isin(datasets_filtered_complete)]

period_performance_filtered = {}
for period in df_filtered['term'].unique():
  period_data_filtered = df_filtered[df_filtered['term'] == period]
  period_performance_filtered[period] = np.exp(np.log(period_data_filtered[model_columns_MASE_linear] + 1).mean()) - 1

df_period_filtered = pd.DataFrame(period_performance_filtered).T

model_order_period_filtered = (np.exp(np.log(df_period_filtered + 1).mean()) - 1).sort_values(ascending=False).index
period_order_filtered = (np.exp(np.log(df_period_filtered + 1).mean(axis=1)) - 1).sort_values(ascending=False).index

df_period_filtered = df_period_filtered.loc[period_order_filtered, model_order_period_filtered]

plt.figure(figsize=(25, 8))
sns.heatmap(df_period_filtered, annot=False, fmt='.2f', cmap='seismic', center=0, vmin=-1, vmax=1)
plt.title('Geometric Mean MASE by Period - filtered (Linear++ baseline)')
plt.xlabel('Models')
plt.ylabel('Periods')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()

plt.savefig("performance_period_filtered_MASE_linear.pdf", format='pdf', bbox_inches='tight')
plt.show()


