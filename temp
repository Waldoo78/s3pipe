# %%
import os
import numpy as np
import pandas as pd
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.cluster.hierarchy import linkage, leaves_list
from scipy.spatial.distance import pdist

# %%
gift_results_folder='/home/wbouainouche/Samformer_distillation/GIFT-Eval/results'

# %%
models = {}
for model in os.listdir(gift_results_folder):
    if os.path.isdir(os.path.join(gift_results_folder, model)):
        csv_path = os.path.join(gift_results_folder, model, 'all_results.csv')
        if os.path.exists(csv_path):
            df = pd.read_csv(csv_path)
            models[model] = df.sort_values('dataset').reset_index(drop=True)

# %% [markdown]
# DATASET DRIVEN ANALYSIS 

# %%
ratio = models['naive']["eval_metrics/MASE[0.5]"] / models['seasonal_naive']["eval_metrics/MASE[0.5]"]

naive_better_mask = ratio < 1
datasets_naive_wins = models['naive'].loc[naive_better_mask, 'dataset']

print("Datasets where Naive beats Seasonal (MASE ratio < 1):")
print(datasets_naive_wins.tolist())

# %%
best_baseline = np.where(ratio < 1, 
                        models['naive']["eval_metrics/MASE[0.5]"], 
                        models['seasonal_naive']["eval_metrics/MASE[0.5]"])

all_ratios = {}
for model in models.keys():
    ratios = models[model]["eval_metrics/MASE[0.5]"] / best_baseline
    all_ratios[model] = ratios

df_ratios = pd.DataFrame(all_ratios)

df_ratios.index = models['naive']['dataset']

df_ratios = df_ratios.reset_index(drop=True)
domain_series = pd.Series(models['naive']['domain'])
num_variates_series = pd.Series(models['naive']['num_variates'])

df_ratios['domain'] = domain_series.values
df_ratios['num_variates'] = num_variates_series.values

df_ratios.index = models['naive']['dataset']

model_columns = [col for col in df_ratios.columns if col not in ['domain', 'num_variates']]


df_ranks = df_ratios[model_columns].rank(axis=1, method='min')

# %%
heatmap_data = df_ratios.drop(columns=['naive', 'seasonal_naive', 'domain', 'num_variates'])

# Clipped at 3.0
heatmap_clipped = heatmap_data.clip(upper=1.5)


distances = pdist(heatmap_clipped, metric='euclidean')
linkage_matrix = linkage(distances, method='ward')
new_order = leaves_list(linkage_matrix)


heatmap_clustered = heatmap_clipped.iloc[new_order]

plt.figure(figsize=(20, 25))
sns.heatmap(heatmap_clustered, 
            annot=False,
            cmap='RdYlGn_r',
            center=0.5,
            vmin=0.01,
            vmax=1.5,
            cbar_kws={'label': 'Normalized MASE (clipped at 2.0)'},
            yticklabels=True)

plt.title('Performance Heatmap: Models vs Datasets (Clustered)', fontsize=16)
plt.xlabel('Models', fontsize=12)
plt.ylabel('Datasets', fontsize=12)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0, fontsize=10)
plt.tight_layout()
plt.show()

# %%
dataset_difficulty_arith = {}

for dataset in heatmap_clustered.index:
    row = heatmap_clustered.loc[dataset]
    dataset_difficulty_arith[dataset] = np.mean(row)

sorted_difficulty = sorted(dataset_difficulty_arith.items(), key=lambda x: x[1], reverse=True)

print("Ranking Difficulty Dataset:")
print("=" * 70)

for i, (dataset, mean_score) in enumerate(sorted_difficulty, 1):
    row = heatmap_clustered.loc[dataset]
    std_score = np.std(row)
    print(f"{i:2d}. {dataset:<40} Mean: {mean_score:.3f} | Std: {std_score:.3f}")

print(f"\nTotal datasets: {len(sorted_difficulty)}")

# %%
datasets_10s = []
for dataset in heatmap_clustered.index:
    if '/' in dataset:
        freq = dataset.split('/')[1]
        if freq == '10T':
            datasets_10s.append(dataset)

print(f"Datasets with 10T frequency: {datasets_10s}")
print(f"Total 10T datasets: {len(datasets_10s)}")


model_performance_10s = {}

model_columns = [col for col in heatmap_clustered.columns if col not in ['domain', 'num_variates']]

for model in model_columns:
    scores_10s = []
    for dataset in datasets_10s:
        score = heatmap_clustered.loc[dataset, model]
        scores_10s.append(score)
    
    model_performance_10s[model] = np.mean(scores_10s)


sorted_models = sorted(model_performance_10s.items(), key=lambda x: x[1])

print("=" * 60)
for i, (model, mean_score) in enumerate(sorted_models, 1):
    print(f"{i:2d}. {model:<20} | Mean: {mean_score:.3f}")

print(f"\nTotal models: {len(sorted_models)}")


best_model = sorted_models[0]
worst_model = sorted_models[-1]



# %%
models_by_year= {
    2015: ['naive', 'seasonal_naive', 'auto_arima', 'auto_ets', 'auto_theta'], 
    
    2017: ['deepar', 'feedforward'],  
    2019: ['N-BEATS', 'tft', 'crossformer'],  
    2022: ['DLinear', 'PatchTST', 'iTransformer', 'tide'],  
    2024: [
        
        'Chronos_small', 'chronos_base', 'chronos_large', 
        'chronos_bolt_small', 'chronos_bolt_base',
        'Moirai_small', 'Moirai_base', 'Moirai_large',
        'timesfm', 'timesfm_2_0_500m',
        'YingLong_6m', 'YingLong_50m', 'YingLong_110m', 'YingLong_300m',
        'TTM-R1-Zeroshot', 'TTM-R2-Zeroshot', 'TTM-R2-Finetuned',
        'visionts', 'Lag-Llama', 'sundial_base_128m', 'Timer', 
        'Toto_Open_Base_1.0', 'TiRex', 'tabpfn_ts', 'tempo_ensemble'
    ]
}

models_by_type = {
    'Statistical': ['naive', 'seasonal_naive', 'auto_arima', 'auto_ets', 'auto_theta'],
    
    'Deep_Learning_Classical': ['deepar', 'feedforward', 'N-BEATS', 'tft'],
    
    'Transformers': ['crossformer', 'PatchTST', 'iTransformer'],
    
    'Modern_Architectures': ['DLinear', 'tide'],  
    
    'Foundation_Models': [
        'Chronos_small', 'chronos_base', 'chronos_large', 
        'chronos_bolt_small', 'chronos_bolt_base',
        'Moirai_small', 'Moirai_base', 'Moirai_large',
        'timesfm', 'timesfm_2_0_500m',
        'YingLong_6m', 'YingLong_50m', 'YingLong_110m', 'YingLong_300m',
        'TTM-R1-Zeroshot', 'TTM-R2-Zeroshot', 'TTM-R2-Finetuned',
        'visionts', 'Lag-Llama', 'sundial_base_128m', 'Timer', 
        'Toto_Open_Base_1.0'
    ],
    
    'Specialized_Advanced': ['TiRex', 'tabpfn_ts', 'tempo_ensemble']  
}


# %%
year_ratios = {}

for year, year_models in models_by_year.items():
    available_models = [m for m in year_models if m in df_ratios.columns]
    
    if available_models:
        year_ratios[year] = df_ratios[available_models].mean(axis=1)

df_evolution = pd.DataFrame(year_ratios)

n_datasets = len(df_evolution.index)
n_cols = 5
n_rows = 20

fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 4*n_rows))
axes = axes.flatten()

for i, dataset in enumerate(df_evolution.index):
    axes[i].plot(df_evolution.columns, df_evolution.loc[dataset], 
                 'o-', linewidth=2, markersize=4)
    axes[i].axhline(y=1.0, color='red', linestyle='--', alpha=0.7)
    axes[i].set_title(dataset, fontsize=10)
    axes[i].grid(True, alpha=0.3)
    axes[i].set_ylabel('Ratio')

for i in range(n_datasets, len(axes)):
    axes[i].remove()

plt.tight_layout()
plt.show()

# %%

df_ratios['frequency'] = df_ratios.index.str.split('/').str[1]

model_columns = [col for col in df_ratios.columns if col not in ['domain', 'num_variates', 'frequency']]


domain_performance = {}
for domain in df_ratios['domain'].unique():
    domain_data = df_ratios[df_ratios['domain'] == domain]
    domain_performance[domain] = domain_data[model_columns].mean()

df_domain = pd.DataFrame(domain_performance).T

plt.figure(figsize=(25, 6))
sns.heatmap(df_domain, 
            annot=True, 
            fmt='.2f',
            cmap='RdYlGn_r',
            center=1.0,
            vmin=0.5,
            vmax=1.5)
plt.title('Performance by Domain')
plt.xlabel('Models')
plt.ylabel('Domains')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()


freq_performance = {}
for freq in df_ratios['frequency'].unique():
    freq_data = df_ratios[df_ratios['frequency'] == freq]
    freq_performance[freq] = freq_data[model_columns].mean()

df_freq = pd.DataFrame(freq_performance).T

plt.figure(figsize=(25, 8))
sns.heatmap(df_freq, 
            annot=True, 
            fmt='.2f',
            cmap='RdYlGn_r',
            center=1.0,
            vmin=0.5,
            vmax=1.5)
plt.title('Performance by frequency')
plt.xlabel('Models')
plt.ylabel('Frequencies')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# %% [markdown]
# MODEL ANALYSIS 

# %%
scores_arith = {}
mean_ranks = {}
std_ranks = {}
min_ranks = {}
q25_ranks = {}
median_ranks = {}
q75_ranks = {}
max_ranks = {}
iqr_ranks = {}
worse_than_baseline = {} 

for model in model_columns:
    ratios = df_ratios[model]
    ranks = df_ranks[model]
    

    scores_arith[model] = ratios.mean()
    mean_ranks[model] = ranks.mean()
    std_ranks[model] = ranks.std()
    # min_ranks[model] = ranks.min()
    q25_ranks[model] = ranks.quantile(0.25)
    median_ranks[model] = ranks.median()
    q75_ranks[model] = ranks.quantile(0.75)
    max_ranks[model] = ranks.max()
    iqr_ranks[model] = q75_ranks[model] - q25_ranks[model]
    
    #Number of times worst than baseline
    worse_than_baseline[model] = (ratios > 1).sum()

print(f"{'Rank':<4} {'Model':<25} {'Arith Score':<10} {'Mean':<6} {'Std':<6} {'Q25':<6} {'Med':<6} {'Q75':<6} {'Max':<4} {'IQR':<6} {'Worse':<5}")
print("-" * 115)


for i, (model, score) in enumerate(sorted(scores_arith.items(), key=lambda x: x[1]), 1):
    arith_score = scores_arith[model]
    mean_rank = mean_ranks[model]
    std_rank = std_ranks[model]
    # min_rank = min_ranks[model]
    q25_rank = q25_ranks[model]
    median_rank = median_ranks[model]
    q75_rank = q75_ranks[model]
    max_rank = max_ranks[model]
    iqr_rank = iqr_ranks[model]
    worse_count = worse_than_baseline[model]
    
    print(f"{i:2d}. {model:<25} {arith_score:<10.3f} {mean_rank:<6.1f} {std_rank:<6.1f} {q25_rank:<6.1f} {median_rank:<6.1f} {q75_rank:<6.1f} {max_rank:<4.0f} {iqr_rank:<6.1f} {worse_count:<5.0f}")

print(f"\nTotal models: {len(scores_arith)}")
print(f"Total datasets: {len(df_ratios)}")


# %%

all_mase = pd.DataFrame({
    model: df['eval_metrics/MASE[0.5]'] 
    for model, df in models.items()
})


ranks = all_mase.rank(axis=1)  

n_models = len(models)
top_30_pct_threshold = 0.3 * n_models  
worst_30_pct_threshold = 0.7 * n_models

results_top = []
results_worst = []
for model in ranks.columns:
    model_ranks = ranks[model]
    
    top_30_rate = (model_ranks <= top_30_pct_threshold).mean()
    worst_30_rate = (model_ranks > worst_30_pct_threshold).mean()
    
    results_top.append((model, top_30_rate))
    results_worst.append((model, worst_30_rate))

results_top.sort(key=lambda x: x[1], reverse=True)
results_worst.sort(key=lambda x: x[1], reverse=True)

print("Models with highest Top 30% rate:")
print("=" * 50)
for i, (model_name, top_30_rate) in enumerate(results_top, 1):
    print(f"{i:2d}. {model_name:<25} | Top30%: {top_30_rate:.1%}")

print(f"\nTotal models: {len(results_top)}")

print("\n\nModels with highest Worst 30% rate:")
print("=" * 50)
for i, (model_name, worst_30_rate) in enumerate(results_worst, 1):
    print(f"{i:2d}. {model_name:<25} | Worst30%: {worst_30_rate:.1%}")

print(f"\nTotal models: {len(results_worst)}")





